{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tal144155/DTS_Project/blob/main/TDS_Project_p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o9quY7wOSoY"
   },
   "source": [
    "# Tabular Data Science - Research Project\n",
    "### Group Members: \n",
    "* Tal Ariel Ziv\n",
    "* Arnon Lutsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwiIVc94PnnJ"
   },
   "source": [
    "#### Introduction\n",
    "Our final project aims to enhance and automate the data visualization process within the data science pipeline. Visualization is a critical step in understanding the data, allowing users to explore distributions, analyze relationships between features and target variables, and gain meaningful insights from different perspectives. By improving and automating this process, we seek to make data exploration more efficient, more intuitive, and accessible. Our solution is an algorithm that automatically analyzes the data for different statistical relations and interesting observations and recommends visualizations based on analysis and a recommendation system.<br>\n",
    "\n",
    "Before we begin, let's install all packages that are needed to run the notebook.\n",
    "#### Installation Guide:\n",
    "1. Download python **version 3.12** (and up). You can use the following [link](https://www.python.org/downloads/).\n",
    "2. Please download all required packages, using the following command (write it in your CMD): `pip install -r requirements.txt`<br><br>\n",
    "<font size=4px>**Now, we are able to begin.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Detection Algorithm:\n",
    "\n",
    "For deeper understanging of the relation detection algorithm, please refer to the pdf with the full explanaion of the project, under Relation Detection Algorithm, section 2.1 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets start analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lTDiV-NNiXy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding The Data\n",
    "Now, let's define the functions that will find the relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set the top 10 relations as the default number of relations returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_RELATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a function that can find which features are in a format of a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_to_date(df):\n",
    "    \"\"\"This function recognize columns that are in the forma of date.\"\"\"\n",
    "    date_pattern = r'^(\\d{4}-\\d{2}-\\d{2})|^(\\d{2}/\\d{2}/\\d{4})|^(\\d{4}/\\d{2}/\\d{2})'\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            if df[column].str.match(date_pattern).any():\n",
    "                try:\n",
    "                    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "                    print(f\"Converted column '{column}' to datetime.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not parse column {column} as datetime. {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks that the dataset exists in the given path, and readx the data using pandas.\n",
    "If the data does not exist in the path, print a message to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset_path, index_col = None):\n",
    "\n",
    "    print(\"- Loading the dataset.\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Error: The file '{dataset_path}' does not exist. Please check the path and try again.\")\n",
    "        return None\n",
    "    if index_col:\n",
    "        df = pd.read_csv(dataset_path, index_col = index_col)\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    column_to_date(df)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines if an integer column is categorical or numeric. \n",
    "If we have very little unique integer values, the column is probably categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_potentially_categorical(column, threshold=0.01):\n",
    "\n",
    "    unique_values = column.nunique()\n",
    "    total_values = len(column)\n",
    "    # check if the percentage of unique values in the column is smaller then the threshold.\n",
    "    if unique_values / total_values < threshold and unique_values < 20:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines the type of each column in our dataset, in order to do smart visualization later.\n",
    "Types we recognize: integer, categorical int, float, boolean, string, categorical string, date, object, other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_types(df):\n",
    "    print(\"- Finding features types in the dataset.\")\n",
    "    column_types = {}\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[column]):\n",
    "            if is_potentially_categorical(df[column]):\n",
    "                column_types[column] = 'categorical_int'\n",
    "            else:\n",
    "                column_types[column] = 'integer'\n",
    "        elif pd.api.types.is_float_dtype(df[column]):\n",
    "            column_types[column] = 'float'\n",
    "        elif pd.api.types.is_bool_dtype(df[column]):\n",
    "            column_types[column] = 'boolean'\n",
    "        elif pd.api.types.is_string_dtype(df[column]):\n",
    "            if is_potentially_categorical(df[column]):\n",
    "                column_types[column] = 'categorical_string'\n",
    "                df[column] = df[column].astype('category')\n",
    "            else:\n",
    "                column_types[column] = 'string'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "            column_types[column] = 'datetime'\n",
    "        elif pd.api.types.is_timedelta64_dtype(df[column]):\n",
    "            column_types[column] = 'timedelta'\n",
    "        elif pd.api.types.is_object_dtype(df[column]):\n",
    "            column_types[column] = 'object'\n",
    "        else:\n",
    "            column_types[column] = 'other'\n",
    "    return column_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all relations with the Target Variable and features with a correlation higher then the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_target_value(df, numerical_columns, target_variable, relations, correlation_threshold=0.5):\n",
    "    # Relations with the Target Variable\n",
    "    print(\"- Checking for correlation with the target variable.\")\n",
    "    correlations = df[numerical_columns].corr()\n",
    "    if target_variable in numerical_columns:\n",
    "        for feature in numerical_columns:\n",
    "            if feature != target_variable:\n",
    "                corr_value = correlations.loc[feature, target_variable]\n",
    "                if abs(corr_value) > correlation_threshold:\n",
    "                    relations.append({\n",
    "                        'attributes': [feature, target_variable],\n",
    "                        'relation_type': 'target_correlation',\n",
    "                        'details': {'correlation_value': corr_value}\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all relations between two features with a correlation higher then the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correlation_relations(df, numerical_columns, target_variable, relations, correlation_threshold=0.5):\n",
    "    # High Correlation Relations (Excluding Target Variable)\n",
    "    print(\"- Checking for correlation.\")\n",
    "    correlations = df[numerical_columns].drop(columns=[target_variable], errors='ignore').corr()\n",
    "    for i, feature1 in enumerate(correlations.columns):\n",
    "        for feature2 in correlations.columns[i + 1:]:\n",
    "            corr_value = correlations.loc[feature1, feature2]\n",
    "            if abs(corr_value) > correlation_threshold:\n",
    "                relations.append(\n",
    "                    {'attributes': [feature1, feature2],\n",
    "                     'relation_type': 'high_correlation',\n",
    "                     'details': {'correlation_value': corr_value}})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function assesses the impact of categorical variables\n",
    "on a numerical target variable using Analysis of Variance\n",
    "(ANOVA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_effects(df, categorical_columns, numerical_columns, target_variable, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for categorical effect.\")\n",
    "    temp_relations = []\n",
    "    if target_variable in numerical_columns:\n",
    "        for cat_feature in categorical_columns:\n",
    "            groups = [df[df[cat_feature] == cat][target_variable].dropna() for cat in df[cat_feature].unique()]\n",
    "            if len(groups) > 1:\n",
    "                f_stat, p_value = f_oneway(*groups)\n",
    "                if p_value < p_value_threshold:\n",
    "                    temp_relations.append(\n",
    "                        {'attributes': [cat_feature, target_variable],\n",
    "                         'relation_type': 'categorical_effect',\n",
    "                         'details': {'p_value': p_value}})\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs the chi 2 test between\n",
    "categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_relationship(df, categorical_columns, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for chi square relation.\")\n",
    "    temp_relations = []\n",
    "    for i, feature1 in enumerate(categorical_columns):\n",
    "        for feature2 in categorical_columns[i + 1:]:\n",
    "            contingency_table = pd.crosstab(df[feature1], df[feature2])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            if p < p_value_threshold:\n",
    "                temp_relations.append(\n",
    "                    {'attributes': [feature1, feature2],\n",
    "                     'relation_type': 'chi_squared',\n",
    "                     'details': {'p_value': p}})\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check for numerical feature trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_numerical_relationship(df, date_columns, numerical_columns, relations, correlation_threshold=0.5):\n",
    "    print(\"- Checking for date with numerical variables.\")\n",
    "    temp_relations = []\n",
    "    for date_col in date_columns:\n",
    "        # Safely convert to datetime and drop NaT values\n",
    "        valid_dates = pd.to_datetime(df[date_col], errors='coerce').dropna()\n",
    "        if valid_dates.empty:\n",
    "            continue\n",
    "        df['time_ordinal'] = valid_dates.map(pd.Timestamp.toordinal)\n",
    "        for num_feature in numerical_columns:\n",
    "            # Only use rows where the date is valid\n",
    "            valid_data = df.loc[valid_dates.index, num_feature].dropna()\n",
    "            if not valid_data.empty:\n",
    "                corr_value = df.loc[valid_data.index, 'time_ordinal'].corr(valid_data)\n",
    "                if abs(corr_value) > correlation_threshold:\n",
    "                    temp_relations.append(\n",
    "                        {'attributes': [date_col, num_feature],\n",
    "                         'relation_type': 'date_numerical_trend',\n",
    "                         'details': {'correlation_value': corr_value}}\n",
    "                    )\n",
    "    temp_relations.sort(key=lambda x: abs(x['details']['correlation_value']), reverse=True)\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check for categorical feature distribution over date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_categorical_relationship(df, date_columns, categorical_columns, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for date with categorical variable.\")\n",
    "    temp_relations = []\n",
    "    for date_col in date_columns:\n",
    "        df['date_period'] = pd.to_datetime(df[date_col]).dt.to_period('M')\n",
    "        for cat_feature in categorical_columns:\n",
    "            contingency_table = pd.crosstab(df['date_period'], df[cat_feature])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            if p < p_value_threshold:\n",
    "                temp_relations.append(\n",
    "                    {'attributes': [date_col, cat_feature],\n",
    "                     'relation_type': 'date_categorical_distribution',\n",
    "                     'details': {'p_value': p}}\n",
    "                )\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes mutual information scores between numerical features to detect strong relationships that\n",
    "may not be identified by linear correlation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_relationships(df, numerical_columns, relations, threshold=0.5):    \n",
    "    print(\"- Checking for non linear relation.\")\n",
    "    for col1 in numerical_columns:\n",
    "        for col2 in numerical_columns:\n",
    "            if col1 != col2:\n",
    "                mi = mutual_info_score(\n",
    "                    pd.qcut(df[col1], 10, duplicates='drop', labels=False), \n",
    "                    pd.qcut(df[col2], 10, duplicates='drop', labels=False)\n",
    "                )\n",
    "                if mi > threshold:\n",
    "                    relations.append({\n",
    "                        'attributes': [col1, col2],\n",
    "                        'relation_type': 'non_linear',\n",
    "                        'details': {'mutual_information': mi}\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function employs a Random Forest Regressor to assess the importance of numerical features in predicting\n",
    "the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_relations(df, numerical_columns, target_variable, relations, top_n=5):\n",
    "    print(\"- Checking for feature importance.\")\n",
    "    \n",
    "    if target_variable in numerical_columns:\n",
    "        X = df[numerical_columns].drop(columns=[target_variable])\n",
    "        y = df[target_variable]\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        feature_importances = sorted(\n",
    "            zip(X.columns, importances), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:top_n]\n",
    "        \n",
    "        importance_details = {\n",
    "            feature: {\n",
    "                'importance_value': importance,\n",
    "                'relative_rank': rank + 1\n",
    "            }\n",
    "            for rank, (feature, importance) in enumerate(feature_importances)\n",
    "        }\n",
    "        \n",
    "        relations.append({\n",
    "            'attributes': [f[0] for f in feature_importances],\n",
    "            'relation_type': 'feature_importance',\n",
    "            'details': {\n",
    "                'importances': importance_details,\n",
    "                'target_variable': target_variable\n",
    "            }\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function dentifies outliers using the Z-score method and analyzes how these outliers influence feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_relationships(df, numerical_columns, relations, z_score_threshold=3.0, min_outlier_ratio=0.01, max_outlier_ratio=0.05, correlation_diff_threshold=0.3):\n",
    "    print(\"- Checking for outliers relation.\")\n",
    "    for col in numerical_columns:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        outliers = df[z_scores > z_score_threshold]\n",
    "        \n",
    "        outlier_ratio = len(outliers) / len(df)\n",
    "        \n",
    "        if min_outlier_ratio < outlier_ratio < max_outlier_ratio:\n",
    "            for other_col in numerical_columns:\n",
    "                if col != other_col:\n",
    "                    outlier_correlation = outliers[col].corr(outliers[other_col])\n",
    "                    normal_correlation = df[col].corr(df[other_col])\n",
    "                    \n",
    "                    if outlier_correlation is not None and normal_correlation is not None:\n",
    "                        if abs(outlier_correlation - normal_correlation) > correlation_diff_threshold:\n",
    "                            relations.append({\n",
    "                                'attributes': [col, other_col],\n",
    "                                'relation_type': 'outlier_pattern',\n",
    "                                'details': {\n",
    "                                    'outlier_correlation': outlier_correlation,\n",
    "                                    'normal_correlation': normal_correlation,\n",
    "                                    'outlier_count': len(outliers)\n",
    "                                }\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function analyses the target variable by detecting outliers\n",
    "and assessing it's distribution against known probability\n",
    "distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_variable_analysis(df, target_variable, relations, z_score_threshold=3.0):\n",
    "    print(\"- Checking for target variable.\")\n",
    "    target_data = df[target_variable]\n",
    "    z_scores = np.abs((target_data - target_data.mean()) / target_data.std())\n",
    "    outliers = target_data[z_scores > z_score_threshold]\n",
    "    \n",
    "    outlier_ratio = len(outliers) / len(target_data)\n",
    "    \n",
    "    distribution_types = ['norm', 'lognorm', 'expon', 'gamma', 'beta']\n",
    "    best_fit = None\n",
    "    best_p_value = 0\n",
    "    \n",
    "    for dist_name in distribution_types:\n",
    "        dist = getattr(stats, dist_name)\n",
    "        params = dist.fit(target_data)\n",
    "        ks_stat, p_value = stats.kstest(target_data, dist_name, args=params)\n",
    "        \n",
    "        if p_value > best_p_value:\n",
    "            best_fit = dist_name\n",
    "            best_p_value = p_value\n",
    "    \n",
    "    relations.append({\n",
    "        'attributes': [target_variable],\n",
    "        'relation_type': 'target_analysis',\n",
    "        'details': {\n",
    "            'outlier_ratio': outlier_ratio,\n",
    "            'outlier_count': len(outliers),\n",
    "            'distribution_type': best_fit,\n",
    "            'distribution_p_value': best_p_value\n",
    "        }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all interesting relations in the dataset and returnes a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(df, target_variable, dataset_types):\n",
    "    relations = []\n",
    "    numerical_columns = [col for col, col_type in dataset_types.items() if col_type in ['integer', 'float']]\n",
    "    categorical_columns = [col for col, col_type in dataset_types.items() if col_type in ['categorical_int', 'categorical_string']]\n",
    "    datetime_columns = [col for col, col_type in dataset_types.items() if col_type == 'datetime']\n",
    "    categorical_int_columns = [col for col, col_type in dataset_types.items() if col_type == 'categorical_int']\n",
    "\n",
    "    # Get the relations with high correlation\n",
    "    correlation_relations(df, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get the relations with the target value\n",
    "    correlation_target_value(df, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get the relations with categorical features\n",
    "    categorical_effects(df, categorical_columns, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get categorical relations using chi-square test\n",
    "    chi_squared_relationship(df, categorical_columns, relations)\n",
    "\n",
    "    # Get relation between date attribute and numerical attributes\n",
    "    date_numerical_relationship(df, datetime_columns, numerical_columns, relations)\n",
    "\n",
    "    # Get relations between date attribute and categorical attributes\n",
    "    date_categorical_relationship(df, datetime_columns, categorical_columns, relations)\n",
    "\n",
    "    # Get non-linear relations between attributes\n",
    "    non_linear_relationships(df, numerical_columns, relations)\n",
    "\n",
    "    # Get attributes importance using random forest\n",
    "    feature_importance_relations(df, numerical_columns + categorical_int_columns, target_variable, relations)\n",
    "\n",
    "    # Get outliers relations\n",
    "    outlier_relationships(df, numerical_columns, relations)\n",
    "    \n",
    "    # Get the distribution of the target variable\n",
    "    target_variable_analysis(df, target_variable, relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the relation detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Final Project/Datasets_Testing/AB_NYC_2019.csv\"\n",
    "# input(\"Please enter the path to your Dataset: \")\n",
    "index_col = \"id\"\n",
    "# input(\"Please enter the index column: \")\n",
    "target_value = \"price\"\n",
    "# input(\"Please enter the name of your target value: \")\n",
    "df = read_data(dataset_path, index_col)\n",
    "if not df is None:\n",
    "    # Understanding the types of columns in the data in order to create better visualizations.\n",
    "    dataset_types = get_column_types(df)\n",
    "    # Calling method to get the relations in the data\n",
    "    find_relations(df, target_value, dataset_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_TYPES = {\n",
    "    \"high_correlation\": {\n",
    "        \"description\": \"Identifies pairs of numerical features that have a strong linear relationship, indicating potential multicollinearity or redundancy in the dataset.\",\n",
    "        \"use_cases\": [\n",
    "            \"Feature selection\",\n",
    "            \"Dimensionality reduction\",\n",
    "            \"Understanding feature interactions\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'target_correlation': {\n",
    "        \"description\": \"Measures the linear relationship between individual features and the target variable, helping to identify the most influential predictors.\",\n",
    "        \"use_cases\": [\n",
    "            \"Feature importance ranking\",\n",
    "            \"Predictive modeling\",\n",
    "            \"Feature selection\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'categorical_effect': {\n",
    "        \"description\": \"Evaluates the statistical significance of categorical variables' impact on a numerical target variable using one-way ANOVA test.\",\n",
    "        \"use_cases\": [\n",
    "            \"Feature significance testing\",\n",
    "            \"Group comparison\",\n",
    "            \"Categorical feature importance\"\n",
    "        ],\n",
    "        \"data_types\": [\"categorical\", \"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'chi_squared': {\n",
    "        \"description\": \"Identifies statistically significant relationships between categorical variables using the chi-squared independence test.\",\n",
    "        \"use_cases\": [\n",
    "            \"Feature dependency analysis\",\n",
    "            \"Categorical variable interaction detection\",\n",
    "            \"Feature selection\"\n",
    "        ],\n",
    "        \"data_types\": [\"categorical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'date_numerical_trend': {\n",
    "        \"description\": \"Detects temporal trends in numerical features by measuring their correlation with time-based attributes.\",\n",
    "        \"use_cases\": [\n",
    "            \"Time series analysis\",\n",
    "            \"Trend identification\",\n",
    "            \"Temporal pattern recognition\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\", \"time series\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'date_categorical_distribution': {\n",
    "        \"description\": \"Analyzes how categorical variable distributions change or are distributed across different time periods.\",\n",
    "        \"use_cases\": [\n",
    "            \"Temporal categorical pattern detection\",\n",
    "            \"Seasonal variation analysis\",\n",
    "            \"Time-based segmentation\"\n",
    "        ],\n",
    "        \"data_types\": [\"categorical\", \"time series\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'non_linear': {\n",
    "        \"description\": \"Identifies complex, non-linear relationships between numerical features using mutual information score.\",\n",
    "        \"use_cases\": [\n",
    "            \"Advanced feature interaction detection\",\n",
    "            \"Non-linear dependency analysis\",\n",
    "            \"Complex relationship mapping\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'feature_importance': {\n",
    "        \"description\": \"Ranks features based on their predictive power using a Random Forest Regressor's feature importance metric.\",\n",
    "        \"use_cases\": [\n",
    "            \"Predictive modeling\",\n",
    "            \"Feature selection\",\n",
    "            \"Model interpretability\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'outlier_pattern': {\n",
    "        \"description\": \"Detects unique correlation patterns among outliers that differ from the overall dataset's correlations.\",\n",
    "        \"use_cases\": [\n",
    "            \"Anomaly detection\",\n",
    "            \"Robust correlation analysis\",\n",
    "            \"Outlier impact assessment\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [2],\n",
    "    },\n",
    "    'cluster_group': {\n",
    "        \"description\": \"Identifies groups of features that exhibit similar clustering characteristics based on their importance within specific clusters.\",\n",
    "        \"use_cases\": [\n",
    "            \"Feature grouping\",\n",
    "            \"Dimensionality reduction\",\n",
    "            \"Structural data understanding\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [1],\n",
    "    },\n",
    "    'target_analysis': {\n",
    "        \"description\": \"Provides a comprehensive analysis of the target variable, including outlier characteristics and distribution properties.\",\n",
    "        \"use_cases\": [\n",
    "            \"Target variable understanding\",\n",
    "            \"Distribution fitting\",\n",
    "            \"Outlier detection\"\n",
    "        ],\n",
    "        \"data_types\": [\"numerical\"],\n",
    "        \"dimensions\": [1],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Save the user ratings to a pickle file for keeping the progress and assessing our model\n",
    "def save_ratings(ratings, file_name):\n",
    "    with open(file_name+'.pkl', 'wb') as f:\n",
    "        pickle.dump(ratings, f)\n",
    "\n",
    "# Load user ratings for collaborative filtering \n",
    "def load_ratings(file_name, rec_types):\n",
    "    file = file_name+'.pkl'\n",
    "    if os.path.isfile(file):\n",
    "        with open( file, 'rb') as f:\n",
    "            ratings = pickle.load(f)\n",
    "    else:\n",
    "        ratings = pd.DataFrame({})\n",
    "        for type in rec_types:\n",
    "            if type not in ratings.columns:\n",
    "                ratings[type] = np.nan\n",
    "\n",
    "    return ratings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for user based content filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CFUB(ratings_pd):\n",
    "    # Get the mean rating for each user\n",
    "    ratings = ratings_pd.to_numpy()\n",
    "    mean_user_rating = ratings_pd.mean(axis=1).to_numpy().reshape(-1, 1)\n",
    "    # calculate the similarity between users\n",
    "    ratings_diff = (ratings - mean_user_rating)\n",
    "    ratings_diff[np.isnan(ratings_diff)]=4\n",
    "    user_similarity = 1-pairwise_distances(ratings_diff, metric='cosine')\n",
    "    pred = mean_user_rating + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for item based content filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CFIB(ratings_pd):\n",
    "    # Get the mean rating for each user\n",
    "    ratings = ratings_pd.to_numpy()\n",
    "    mean_user_rating = ratings_pd.mean(axis=1).to_numpy().reshape(-1, 1)\n",
    "    # calculate the similarity between visualizations\n",
    "    ratings_diff = (ratings - mean_user_rating)\n",
    "    ratings_diff[np.isnan(ratings_diff)]=4\n",
    "    vis_similarity = 1-pairwise_distances(ratings_diff, metric='cosine')\n",
    "    pred = mean_user_rating + vis_similarity.dot(ratings_diff) / np.array([np.abs(vis_similarity).sum(axis=1)]).T\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum of two predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pred(pred1, pred2, w1 = 0.5, w2 = 0.5):\n",
    "    # Replace NaN values with 0\n",
    "    pred1 = np.nan_to_num(pred1, nan=4.0)\n",
    "    pred2 = np.nan_to_num(pred2, nan=4.0)\n",
    "\n",
    "    return w1 * pred1 + w2 * pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the score for a relation of any type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score(value, metric_type):\n",
    "    \"\"\"\n",
    "    Normalize different types of statistical measures to a 1-5 scale.\n",
    "    \"\"\"\n",
    "    # Normalization strategies for different metric types\n",
    "    normalization_strategies = {\n",
    "        'high_correlation': {\n",
    "            'abs_range': (0.5, 1.0),  # Correlation values are between -1 and 1\n",
    "            'percentile_thresholds': [0.5, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "        'target_correlation': {\n",
    "            'abs_range': (0.5, 1.0),  # Correlation with target variable\n",
    "            'percentile_thresholds': [0.5, 0.6, 0.7, 0.9]\n",
    "        },\n",
    "        'categorical_effect': {\n",
    "            'abs_range': (0, 0.05),  # P-values, lower is stronger\n",
    "            'percentile_thresholds': [0.05, 0.02, 0.01, 0.009]\n",
    "        },\n",
    "        'chi_squared': {\n",
    "            'abs_range': (0, 0.05),  # P-values, lower is stronger\n",
    "            'percentile_thresholds': [0.05, 0.02, 0.01, 0.009]\n",
    "        },\n",
    "        'date_numerical_trend': {\n",
    "            'abs_range': (0.5, 1.0),  # Correlation values\n",
    "            'percentile_thresholds': [0.5, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "        'date_categorical_distribution':{\n",
    "            'abs_range': (0, 0.05),  # P-values, lower is stronger\n",
    "            'percentile_thresholds': [0.05, 0.02, 0.01, 0.009]\n",
    "        },\n",
    "        'non_linear': {\n",
    "            'abs_range': (0.5, 1.0),  # Mutual information score\n",
    "            'percentile_thresholds': [0.5, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "        'feature_importance': {\n",
    "            'abs_range': (0, 1.0),  # Feature importance values\n",
    "            'percentile_thresholds': [0.2, 0.4, 0.6, 0.8]\n",
    "        },\n",
    "        'outlier_pattern': {\n",
    "            'abs_range': (0.5, 1.0),  # Correlation differences\n",
    "            'percentile_thresholds': [0.5, 0.7, 0.8, 0.9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    if metric_type == 'cluster_group':\n",
    "        # Normalize based on number of features in cluster or importance\n",
    "        return min(max(1, int(value * 5)), 5)\n",
    "    elif metric_type == 'target_analysis':\n",
    "        # Normalize outlier ratio or distribution significance\n",
    "        return min(max(1, int(value * 5)), 5)\n",
    "    \n",
    "    # We'll set the middle score as the default\n",
    "    if metric_type not in normalization_strategies:\n",
    "        return 3  \n",
    "    \n",
    "    strategy = normalization_strategies[metric_type]\n",
    "    \n",
    "    # Absolute value for signed metrics\n",
    "    abs_value = abs(value)\n",
    "    \n",
    "    \n",
    "    # Value-based normalization\n",
    "    min_val, max_val = strategy['abs_range']\n",
    "    \n",
    "    # Normalize to 1-5 range\n",
    "    if abs_value <= min_val:\n",
    "        return 1\n",
    "    elif abs_value >= max_val:\n",
    "        return 5\n",
    "    else:\n",
    "        normalized = 1 + 4 * (abs_value - min_val) / (max_val - min_val)\n",
    "        return int(min(max(normalized, 1), 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets the score for each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_scores(relations):\n",
    "    \"\"\"\n",
    "    Apply strength normalization to all relations.\n",
    "    \"\"\"\n",
    "    for relation in relations:\n",
    "        if relation['relation_type'] in {'high_correlation', 'target_correlation','date_numerical_trend',}:\n",
    "            value = relation['details']['correlation_value']\n",
    "        elif relation['relation_type'] in {'categorical_effect', 'date_categorical_distribution', 'chi_squared',  }:\n",
    "            value = relation['details']['p_value']\n",
    "        elif relation['relation_type'] == 'non_linear':\n",
    "            value = relation['details']['mutual_information']\n",
    "        elif relation['relation_type'] == 'outlier_pattern':\n",
    "            value = relation['details']['outlier_correlation']\n",
    "        else:\n",
    "            value = 1       \n",
    "        \n",
    "        relation['score'] = normalize_score(\n",
    "            value, \n",
    "            relation['relation_type']\n",
    "        )\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a pandas DF with the top relations of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_relations(relations):\n",
    "    algo_rec_df = pd.DataFrame({})\n",
    "    top_relations = {}\n",
    "\n",
    "    for i, rel in enumerate(relations):\n",
    "        type = rel['relation_type']\n",
    "        if not type in top_relations:\n",
    "            top_relations[type] = {'score': rel['score'], 'index': i}\n",
    "\n",
    "    for type in RELATION_TYPES:\n",
    "        score = 0\n",
    "        indx = -1\n",
    "        if type in top_relations:\n",
    "            score = top_relations[type]['score']\n",
    "            indx = top_relations[type]['index']\n",
    "        algo_rec_df.loc[0, type] = score\n",
    "        algo_rec_df.loc[1, type] = indx\n",
    "\n",
    "    return algo_rec_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
