{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tal144155/DTS_Project/blob/main/TDS_Project_p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o9quY7wOSoY"
   },
   "source": [
    "# Tabular Data Science - Research Project\n",
    "### Group Members: \n",
    "* Tal Ariel Ziv\n",
    "* Arnon Lutsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwiIVc94PnnJ"
   },
   "source": [
    "#### Introduction\n",
    "Our final project aims to enhance and automate the data visualization process within the data science pipeline. Visualization is a critical step in understanding the data, allowing users to explore distributions, analyze relationships between features and target variables, and gain meaningful insights from different perspectives. By improving and automating this process, we seek to make data exploration more efficient, more intuitive, and accessible. Our solution is an algorithm that automatically analyzes the data for different statistical relations and interesting observations and recommends visualizations based on analysis and a recommendation system.<br>\n",
    "\n",
    "Before we begin, let's install all packages that are needed to run the notebook.\n",
    "#### Installation Guide:\n",
    "1. Download python **version 3.12** (and up). You can use the following [link](https://www.python.org/downloads/).\n",
    "2. Please download all required packages, using the following command (write it in your CMD): `pip install -r requirements.txt`<br><br>\n",
    "<font size=4px>**Now, we are able to begin.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Detection Algorithm:\n",
    "\n",
    "For deeper understanging of the relation detection algorithm, please refer to the pdf with the full explanaion of the project, under Relation Detection Algorithm, section 2.1 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets start analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lTDiV-NNiXy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding The Data\n",
    "Now, let's define the functions that will find the relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set the top 10 relations as the default number of relations returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_RELATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a function that can find which features are in a format of a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_to_date(df):\n",
    "    \"\"\"This function recognize columns that are in the forma of date.\"\"\"\n",
    "    date_pattern = r'^(\\d{4}-\\d{2}-\\d{2})|^(\\d{2}/\\d{2}/\\d{4})|^(\\d{4}/\\d{2}/\\d{2})'\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            if df[column].str.match(date_pattern).any():\n",
    "                try:\n",
    "                    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "                    print(f\"Converted column '{column}' to datetime.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not parse column {column} as datetime. {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks that the dataset exists in the given path, and readx the data using pandas.\n",
    "If the data does not exist in the path, print a message to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset_path, index_col = None):\n",
    "\n",
    "    print(\"- Loading the dataset.\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Error: The file '{dataset_path}' does not exist. Please check the path and try again.\")\n",
    "        return None\n",
    "    if index_col:\n",
    "        df = pd.read_csv(dataset_path, index_col = index_col)\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    column_to_date(df)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines if an integer column is categorical or numeric. \n",
    "If we have very little unique integer values, the column is probably categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_potentially_categorical(column, threshold=0.01):\n",
    "\n",
    "    unique_values = column.nunique()\n",
    "    total_values = len(column)\n",
    "    # check if the percentage of unique values in the column is smaller then the threshold.\n",
    "    if unique_values / total_values < threshold and unique_values < 20:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines the type of each column in our dataset, in order to do smart visualization later.\n",
    "Types we recognize: integer, categorical int, float, boolean, string, categorical string, date, object, other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_types(df):\n",
    "    print(\"- Finding features types in the dataset.\")\n",
    "    column_types = {}\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[column]):\n",
    "            if is_potentially_categorical(df[column]):\n",
    "                column_types[column] = 'categorical_int'\n",
    "            else:\n",
    "                column_types[column] = 'integer'\n",
    "        elif pd.api.types.is_float_dtype(df[column]):\n",
    "            column_types[column] = 'float'\n",
    "        elif pd.api.types.is_bool_dtype(df[column]):\n",
    "            column_types[column] = 'boolean'\n",
    "        elif pd.api.types.is_string_dtype(df[column]):\n",
    "            if is_potentially_categorical(df[column]):\n",
    "                column_types[column] = 'categorical_string'\n",
    "                df[column] = df[column].astype('category')\n",
    "            else:\n",
    "                column_types[column] = 'string'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "            column_types[column] = 'datetime'\n",
    "        elif pd.api.types.is_timedelta64_dtype(df[column]):\n",
    "            column_types[column] = 'timedelta'\n",
    "        elif pd.api.types.is_object_dtype(df[column]):\n",
    "            column_types[column] = 'object'\n",
    "        else:\n",
    "            column_types[column] = 'other'\n",
    "    return column_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all relations with the Target Variable and features with a correlation higher then the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_target_value(df, numerical_columns, target_variable, relations, correlation_threshold=0.5):\n",
    "    # Relations with the Target Variable\n",
    "    print(\"- Checking for correlation with the target variable.\")\n",
    "    correlations = df[numerical_columns].corr()\n",
    "    if target_variable in numerical_columns:\n",
    "        for feature in numerical_columns:\n",
    "            if feature != target_variable:\n",
    "                corr_value = correlations.loc[feature, target_variable]\n",
    "                if abs(corr_value) > correlation_threshold:\n",
    "                    relations.append({\n",
    "                        'attributes': [feature, target_variable],\n",
    "                        'relation_type': 'target_correlation',\n",
    "                        'details': {'correlation_value': corr_value}\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all relations between two features with a correlation higher then the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correlation_relations(df, numerical_columns, target_variable, relations, correlation_threshold=0.5):\n",
    "    # High Correlation Relations (Excluding Target Variable)\n",
    "    print(\"- Checking for correlation.\")\n",
    "    correlations = df[numerical_columns].drop(columns=[target_variable], errors='ignore').corr()\n",
    "    for i, feature1 in enumerate(correlations.columns):\n",
    "        for feature2 in correlations.columns[i + 1:]:\n",
    "            corr_value = correlations.loc[feature1, feature2]\n",
    "            if abs(corr_value) > correlation_threshold:\n",
    "                relations.append(\n",
    "                    {'attributes': [feature1, feature2],\n",
    "                     'relation_type': 'high_correlation',\n",
    "                     'details': {'correlation_value': corr_value}})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function assesses the impact of categorical variables\n",
    "on a numerical target variable using Analysis of Variance\n",
    "(ANOVA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_effects(df, categorical_columns, numerical_columns, target_variable, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for categorical effect.\")\n",
    "    temp_relations = []\n",
    "    if target_variable in numerical_columns:\n",
    "        for cat_feature in categorical_columns:\n",
    "            groups = [df[df[cat_feature] == cat][target_variable].dropna() for cat in df[cat_feature].unique()]\n",
    "            if len(groups) > 1:\n",
    "                f_stat, p_value = f_oneway(*groups)\n",
    "                if p_value < p_value_threshold:\n",
    "                    temp_relations.append(\n",
    "                        {'attributes': [cat_feature, target_variable],\n",
    "                         'relation_type': 'categorical_effect',\n",
    "                         'details': {'p_value': p_value}})\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs the chi 2 test between\n",
    "categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_relationship(df, categorical_columns, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for chi square relation.\")\n",
    "    temp_relations = []\n",
    "    for i, feature1 in enumerate(categorical_columns):\n",
    "        for feature2 in categorical_columns[i + 1:]:\n",
    "            contingency_table = pd.crosstab(df[feature1], df[feature2])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            if p < p_value_threshold:\n",
    "                temp_relations.append(\n",
    "                    {'attributes': [feature1, feature2],\n",
    "                     'relation_type': 'chi_squared',\n",
    "                     'details': {'p_value': p}})\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check for numerical feature trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_numerical_relationship(df, date_columns, numerical_columns, relations, correlation_threshold=0.5):\n",
    "    print(\"- Checking for date with numerical variables.\")\n",
    "    temp_relations = []\n",
    "    for date_col in date_columns:\n",
    "        # Safely convert to datetime and drop NaT values\n",
    "        valid_dates = pd.to_datetime(df[date_col], errors='coerce').dropna()\n",
    "        if valid_dates.empty:\n",
    "            continue\n",
    "        df['time_ordinal'] = valid_dates.map(pd.Timestamp.toordinal)\n",
    "        for num_feature in numerical_columns:\n",
    "            # Only use rows where the date is valid\n",
    "            valid_data = df.loc[valid_dates.index, num_feature].dropna()\n",
    "            if not valid_data.empty:\n",
    "                corr_value = df.loc[valid_data.index, 'time_ordinal'].corr(valid_data)\n",
    "                if abs(corr_value) > correlation_threshold:\n",
    "                    temp_relations.append(\n",
    "                        {'attributes': [date_col, num_feature],\n",
    "                         'relation_type': 'date_numerical_trend',\n",
    "                         'details': {'correlation_value': corr_value}}\n",
    "                    )\n",
    "    temp_relations.sort(key=lambda x: abs(x['details']['correlation_value']), reverse=True)\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check for categorical feature distribution over date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_categorical_relationship(df, date_columns, categorical_columns, relations, p_value_threshold=0.05):\n",
    "    print(\"- Checking for date with categorical variable.\")\n",
    "    temp_relations = []\n",
    "    for date_col in date_columns:\n",
    "        df['date_period'] = pd.to_datetime(df[date_col]).dt.to_period('M')\n",
    "        for cat_feature in categorical_columns:\n",
    "            contingency_table = pd.crosstab(df['date_period'], df[cat_feature])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            if p < p_value_threshold:\n",
    "                temp_relations.append(\n",
    "                    {'attributes': [date_col, cat_feature],\n",
    "                     'relation_type': 'date_categorical_distribution',\n",
    "                     'details': {'p_value': p}}\n",
    "                )\n",
    "    temp_relations.sort(key=lambda x: x['details']['p_value'])\n",
    "    relations.extend(temp_relations[:TOP_N_RELATIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes mutual information scores between numerical features to detect strong relationships that\n",
    "may not be identified by linear correlation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_relationships(df, numerical_columns, relations, threshold=0.5):    \n",
    "    print(\"- Checking for non linear relation.\")\n",
    "    for col1 in numerical_columns:\n",
    "        for col2 in numerical_columns:\n",
    "            if col1 != col2:\n",
    "                mi = mutual_info_score(\n",
    "                    pd.qcut(df[col1], 10, duplicates='drop', labels=False), \n",
    "                    pd.qcut(df[col2], 10, duplicates='drop', labels=False)\n",
    "                )\n",
    "                if mi > threshold:\n",
    "                    relations.append({\n",
    "                        'attributes': [col1, col2],\n",
    "                        'relation_type': 'non_linear',\n",
    "                        'details': {'mutual_information': mi}\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function employs a Random Forest Regressor to assess the importance of numerical features in predicting\n",
    "the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_relations(df, numerical_columns, target_variable, relations, top_n=5):\n",
    "    print(\"- Checking for feature importance.\")\n",
    "    \n",
    "    if target_variable in numerical_columns:\n",
    "        X = df[numerical_columns].drop(columns=[target_variable])\n",
    "        y = df[target_variable]\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        feature_importances = sorted(\n",
    "            zip(X.columns, importances), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:top_n]\n",
    "        \n",
    "        importance_details = {\n",
    "            feature: {\n",
    "                'importance_value': importance,\n",
    "                'relative_rank': rank + 1\n",
    "            }\n",
    "            for rank, (feature, importance) in enumerate(feature_importances)\n",
    "        }\n",
    "        \n",
    "        relations.append({\n",
    "            'attributes': [f[0] for f in feature_importances],\n",
    "            'relation_type': 'feature_importance',\n",
    "            'details': {\n",
    "                'importances': importance_details,\n",
    "                'target_variable': target_variable\n",
    "            }\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function dentifies outliers using the Z-score method and analyzes how these outliers influence feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_relationships(df, numerical_columns, relations, z_score_threshold=3.0, min_outlier_ratio=0.01, max_outlier_ratio=0.05, correlation_diff_threshold=0.3):\n",
    "    print(\"- Checking for outliers relation.\")\n",
    "    for col in numerical_columns:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        outliers = df[z_scores > z_score_threshold]\n",
    "        \n",
    "        outlier_ratio = len(outliers) / len(df)\n",
    "        \n",
    "        if min_outlier_ratio < outlier_ratio < max_outlier_ratio:\n",
    "            for other_col in numerical_columns:\n",
    "                if col != other_col:\n",
    "                    outlier_correlation = outliers[col].corr(outliers[other_col])\n",
    "                    normal_correlation = df[col].corr(df[other_col])\n",
    "                    \n",
    "                    if outlier_correlation is not None and normal_correlation is not None:\n",
    "                        if abs(outlier_correlation - normal_correlation) > correlation_diff_threshold:\n",
    "                            relations.append({\n",
    "                                'attributes': [col, other_col],\n",
    "                                'relation_type': 'outlier_pattern',\n",
    "                                'details': {\n",
    "                                    'outlier_correlation': outlier_correlation,\n",
    "                                    'normal_correlation': normal_correlation,\n",
    "                                    'outlier_count': len(outliers)\n",
    "                                }\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function analyses the target variable by detecting outliers\n",
    "and assessing it's distribution against known probability\n",
    "distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_variable_analysis(df, target_variable, relations, z_score_threshold=3.0):\n",
    "    print(\"- Checking for target variable.\")\n",
    "    target_data = df[target_variable]\n",
    "    z_scores = np.abs((target_data - target_data.mean()) / target_data.std())\n",
    "    outliers = target_data[z_scores > z_score_threshold]\n",
    "    \n",
    "    outlier_ratio = len(outliers) / len(target_data)\n",
    "    \n",
    "    distribution_types = ['norm', 'lognorm', 'expon', 'gamma', 'beta']\n",
    "    best_fit = None\n",
    "    best_p_value = 0\n",
    "    \n",
    "    for dist_name in distribution_types:\n",
    "        dist = getattr(stats, dist_name)\n",
    "        params = dist.fit(target_data)\n",
    "        ks_stat, p_value = stats.kstest(target_data, dist_name, args=params)\n",
    "        \n",
    "        if p_value > best_p_value:\n",
    "            best_fit = dist_name\n",
    "            best_p_value = p_value\n",
    "    \n",
    "    relations.append({\n",
    "        'attributes': [target_variable],\n",
    "        'relation_type': 'target_analysis',\n",
    "        'details': {\n",
    "            'outlier_ratio': outlier_ratio,\n",
    "            'outlier_count': len(outliers),\n",
    "            'distribution_type': best_fit,\n",
    "            'distribution_p_value': best_p_value\n",
    "        }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all interesting relations in the dataset and returnes a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(df, target_variable, dataset_types):\n",
    "    relations = []\n",
    "    numerical_columns = [col for col, col_type in dataset_types.items() if col_type in ['integer', 'float']]\n",
    "    categorical_columns = [col for col, col_type in dataset_types.items() if col_type in ['categorical_int', 'categorical_string']]\n",
    "    datetime_columns = [col for col, col_type in dataset_types.items() if col_type == 'datetime']\n",
    "    categorical_int_columns = [col for col, col_type in dataset_types.items() if col_type == 'categorical_int']\n",
    "\n",
    "    # Get the relations with high correlation\n",
    "    correlation_relations(df, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get the relations with the target value\n",
    "    correlation_target_value(df, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get the relations with categorical features\n",
    "    categorical_effects(df, categorical_columns, numerical_columns, target_variable, relations)\n",
    "\n",
    "    # Get categorical relations using chi-square test\n",
    "    chi_squared_relationship(df, categorical_columns, relations)\n",
    "\n",
    "    # Get relation between date attribute and numerical attributes\n",
    "    date_numerical_relationship(df, datetime_columns, numerical_columns, relations)\n",
    "\n",
    "    # Get relations between date attribute and categorical attributes\n",
    "    date_categorical_relationship(df, datetime_columns, categorical_columns, relations)\n",
    "\n",
    "    # Get non-linear relations between attributes\n",
    "    non_linear_relationships(df, numerical_columns, relations)\n",
    "\n",
    "    # Get attributes importance using random forest\n",
    "    feature_importance_relations(df, numerical_columns + categorical_int_columns, target_variable, relations)\n",
    "\n",
    "    # Get outliers relations\n",
    "    outlier_relationships(df, numerical_columns, relations)\n",
    "    \n",
    "    # Get the distribution of the target variable\n",
    "    target_variable_analysis(df, target_variable, relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the relation detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Final Project/Datasets_Testing/AB_NYC_2019.csv\"\n",
    "# input(\"Please enter the path to your Dataset: \")\n",
    "index_col = \"id\"\n",
    "# input(\"Please enter the index column: \")\n",
    "target_value = \"price\"\n",
    "# input(\"Please enter the name of your target value: \")\n",
    "df = read_data(dataset_path, index_col)\n",
    "if not df is None:\n",
    "    # Understanding the types of columns in the data in order to create better visualizations.\n",
    "    dataset_types = get_column_types(df)\n",
    "    # Calling method to get the relations in the data\n",
    "    find_relations(df, target_value, dataset_types)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
